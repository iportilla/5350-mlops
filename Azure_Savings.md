 ## How to Reduce Cost When Using Azure ML Deployments

Azure ML can get expensive very quickly if not configured correctly.
Here are the top cost-saving strategies, ordered by impact.

â¸»

### 1. Use Serverless Endpoints for Inference (BIGGEST SAVINGS)

Instead of provisioning compute (VMs running 24/7), serverless charges only per request.

When to use serverless:

âœ” Classroom demos
âœ” Light workloads
âœ” Occasional model testing
âœ” Low-volume inference

Why it saves money:
	â€¢	No idle running VMs
	â€¢	No need to manage scaling
	â€¢	Costs approach zero when unused
	â€¢	No GPU charges unless you explicitly choose GPU serverless

â¸»

### 2. Avoid Persistent Compute for Deployments

If you deploy using Managed Online Endpoints with compute, Azure allocates a VM like:
	â€¢	Standard_F2s
	â€¢	Standard_DS3
	â€¢	GPU nodes ðŸ˜± (EXPENSIVE)

These VMs run 24/7 until you delete the endpoint.

âœ” After class â†’ DELETE the endpoint
âœ” Or scale to 0 instances if supported
âœ” Use smallest SKU (F2s_v2 or B-series)

â¸»

### 3. Use Lower-Cost SKU for Training/Compute

When you must use compute:
	â€¢	Prefer CPU clusters unless GPU needed
	â€¢	Use F2s_v2, B2s, B4ms
	â€¢	Use low priority or spot instances for huge discounts

â¸»

### 4. Enable Autoscaling Wisely

Default autoscaling is often wasteful.

Recommended autoscale for demos:
	â€¢	Min nodes = 0
	â€¢	Max nodes = 1â€“2
	â€¢	Idle time before scale down = 5â€“10 min

This alone can save 50â€“80%.

â¸»

###  5. Turn Off Compute Instances After Use

Compute Instances (like notebook VMs) are a hidden cost trap.

Always:
âœ” Stop the instance when done
âœ” Set policies to auto-stop after X hours
âœ” Use shared compute for the whole class

â¸»

###  6. Use Free or Included Models Where Possible

Azure sometimes provides â€œincluded quotaâ€ on certain models.
Prefer:
	â€¢	Azure-OpenAI small models
	â€¢	Built-in classical models
	â€¢	Pretrained MLflow models

Avoid expensive foundation models unless needed.

â¸»

###  7. Use Batch Endpoints Instead of Online Endpoints

For anything not real-time:
âœ” Batch can run on spot compute
âœ” Cost is >80% cheaper than online endpoints
âœ” No VM running when idle

â¸»

### 8. Delete Everything After Class

The top cause of unexpected bills:
forgotten endpoints + forgotten compute.

Teach students to clean up:
	â€¢	Endpoints
	â€¢	Managed deployments
	â€¢	Compute clusters
	â€¢	Compute instances
	â€¢	Storage logs

â¸»

### Quick Classroom Example

If students deploy a model to Standard_F2s_v2:
	â€¢	$0.14/hr
	â€¢	$3.36/day
	â€¢	~$100/month (for one student!)

But serverless:
	â€¢	$0 when idle
	â€¢	Only pay for requests
	â€¢	A classroom can cost < $1 total

This is why serverless is preferred for teaching and MLOps inference workloads.

â¸»

### Summary for Students (One Slide Version)

Best Cost-Saving Practices in Azure ML
	1.	Use serverless endpoints for inference
	2.	Choose smallest compute SKU
	3.	Set autoscaling with min=0
	4.	Use batch endpoints for non-real-time workloads
	5.	Stop or delete compute after use
	6.	Prefer CPU over GPU
	7.	Use spot instances for training
	8.	Clean up endpoints after class
